{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep.models import build_model\n",
    "from deep.reid_model_factory import (\n",
    "    get_model_name,\n",
    "    get_model_url,\n",
    "    load_pretrained_weights,\n",
    "    show_downloadeable_models,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "from os.path import exists as file_exists\n",
    "from pathlib import Path\n",
    "import gdown\n",
    "import torchvision.transforms as T\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Metric_Interface(object):\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, output):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyparsing\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchmetrics.functional import accuracy\n",
    "# from metrics.metric import Metric_Interface\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class Person_Reid(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        evaluator: None,\n",
    "        # cfg: None,\n",
    "    ):\n",
    "        super(Person_Reid, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.evaluator = evaluator\n",
    "        # self.cfg = cfg\n",
    "        self.predict_ouputs = dict\n",
    "        self.start_time = time.time()\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, pid = batch\n",
    "        score, feat = self.model(img)\n",
    "        loss = self.loss_fn(score, feat, pid)\n",
    "        \n",
    "        if isinstance(score, list):\n",
    "            acc = (score[0].max(1)[1] == pid).float().mean()\n",
    "        else:\n",
    "            acc = (score.max(1)[1] == pid).float().mean()\n",
    "        \n",
    "        self.log(f\"Train loss\", loss, prog_bar=True)\n",
    "        self.log(f\"Train accuracy\", acc, prog_bar=True)\n",
    "\n",
    "        return {\"loss\": loss, \"acc\": acc}\n",
    "\n",
    "    def evaluate(self, batch, stage):\n",
    "        img, pid, camid, img_path = batch\n",
    "        feat = self.model(img)\n",
    "        self.evaluator.update([feat.cpu(), pid, camid])\n",
    "        return\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.evaluate(batch, \"val\")\n",
    "\n",
    "    # def predict_step(self, batch: pyparsing.Any, batch_idx: int, dataloader_idx: int = 0) -> pyparsing.Any:\n",
    "    #     return dict(\n",
    "    #         zip(batch[1], super().predict_step(batch[0], batch_idx, dataloader_idx))\n",
    "    #     )\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        print(\"Start training on epoch:\", self.current_epoch)\n",
    "\n",
    "    def training_epoch_end(self, outputs) -> None:\n",
    "        loss = torch.mean(torch.stack([o[\"loss\"] for o in outputs], dim=0))\n",
    "        acc = torch.mean(torch.stack([o[\"acc\"] for o in outputs], dim=0)) * 100\n",
    "        print(\n",
    "            \"Epoch: {:.1f},Train accuracy: {:.5f}, Train loss: {:.5f}\".format(\n",
    "                self.current_epoch, acc, loss\n",
    "            )\n",
    "        )\n",
    "        print(\"Learning rate:\", self.scheduler.get_lr()[0])\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        print(\"Calulating the acc, cmc, mAP\")\n",
    "\n",
    "        cmc, mAP = self.evaluator.compute()\n",
    "\n",
    "        self.log(\"Val_CMC@rank1\", cmc[0])\n",
    "        self.log(\"Val_CMC@rank5\", cmc[4])\n",
    "        self.log(\"Val_mAP\", mAP)\n",
    "\n",
    "        print(\"Validation result:\")\n",
    "        print(\n",
    "            \"Validation epoch: {:.1f}, CMC@rank1: {:.5f}%, CMC@rank5: {:.5f}%, mAP: {:.5f}%\".format(\n",
    "                self.current_epoch, cmc[0] * 100, cmc[4] * 100, mAP * 100\n",
    "            )\n",
    "        )\n",
    "        self.evaluator.reset()\n",
    "        # if time.time() - self.start_time > 36000:\n",
    "        #     print(\"OUT of training time, stop!!\")\n",
    "        #     exit(0)\n",
    "        return\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler}\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        print(\"--------\")\n",
    "        print(\"Start validation\")\n",
    "\n",
    "    # def on_train_start(self):\n",
    "    #     print(\"Saving directory: \", self.logger.log_dir)\n",
    "    #     import ruamel.yaml as yaml\n",
    "\n",
    "    #     if not os.path.exists(self.logger.log_dir):\n",
    "    #         os.makedirs(self.logger.log_dir)\n",
    "\n",
    "    #     with open(self.logger.log_dir + \"/settings.yaml\", \"w+\") as yml:\n",
    "    #         yaml.dump(self.cfg, yml, allow_unicode=True, Dumper=yaml.RoundTripDumper)\n",
    "\n",
    "    # def lr_scheduler_step(self, scheduler, optimizer_idx, metric):\n",
    "    #     if self.cfg[\"SOLVER\"][\"WARMUP_METHOD\"] == \"cosine\":\n",
    "    #         scheduler.step(\n",
    "    #             epoch=self.current_epoch\n",
    "    #         )  # timm's scheduler need the epoch value\n",
    "    #     else:\n",
    "    #         scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kadirnar: I added export_formats to the function\n",
    "def export_formats():\n",
    "    # YOLOv5 export formats\n",
    "    x = [\n",
    "        [\"PyTorch\", \"-\", \".pt\", True, True],\n",
    "        [\"PyTorch\", \"-\", \".ckpt\", True, True],\n",
    "        [\"TorchScript\", \"torchscript\", \".torchscript\", True, True],\n",
    "        [\"ONNX\", \"onnx\", \".onnx\", True, True],\n",
    "        [\"OpenVINO\", \"openvino\", \"_openvino_model\", True, False],\n",
    "        [\"TensorRT\", \"engine\", \".engine\", False, True],\n",
    "        [\"CoreML\", \"coreml\", \".mlmodel\", True, False],\n",
    "        [\"TensorFlow SavedModel\", \"saved_model\", \"_saved_model\", True, True],\n",
    "        [\"TensorFlow GraphDef\", \"pb\", \".pb\", True, True],\n",
    "        [\"TensorFlow Lite\", \"tflite\", \".tflite\", True, False],\n",
    "        [\"TensorFlow Edge TPU\", \"edgetpu\", \"_edgetpu.tflite\", False, False],\n",
    "        [\"TensorFlow.js\", \"tfjs\", \"_web_model\", False, False],\n",
    "        [\"PaddlePaddle\", \"paddle\", \"_paddle_model\", True, True],\n",
    "    ]\n",
    "    return pd.DataFrame(x, columns=[\"Format\", \"Argument\", \"Suffix\", \"CPU\", \"GPU\"])\n",
    "\n",
    "def check_suffix(file=\"yolov8x.pt\", suffix=(\".pt\",), msg=\"\"):\n",
    "    # Check file(s) for acceptable suffix\n",
    "    if file and suffix:\n",
    "        if isinstance(suffix, str):\n",
    "            suffix = [suffix]\n",
    "        for f in file if isinstance(file, (list, tuple)) else [file]:\n",
    "            s = Path(f).suffix.lower()  # file suffix\n",
    "            if len(s):\n",
    "                assert s in suffix, f\"{msg}{f} acceptable suffix is {suffix}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReIDDetectMultiBackend(nn.Module):\n",
    "    # ReID models MultiBackend class for python inference on various backends\n",
    "    def __init__(self, weights=\"vit_base_patch16_224_TransReID.ckpt\", device=torch.device(\"cpu\"), fp16=False):\n",
    "        super().__init__()\n",
    "\n",
    "        w = weights[0] if isinstance(weights, list) else weights\n",
    "        (\n",
    "            self.pt,\n",
    "            self.ckpt,\n",
    "            self.jit,\n",
    "            self.onnx,\n",
    "            self.xml,\n",
    "            self.engine,\n",
    "            self.coreml,\n",
    "            self.saved_model,\n",
    "            self.pb,\n",
    "            self.tflite,\n",
    "            self.edgetpu,\n",
    "            self.tfjs,\n",
    "        ) = self.model_type(\n",
    "            w\n",
    "        )  # get backend\n",
    "        self.fp16 = fp16\n",
    "        self.fp16 &= self.pt or self.jit or self.engine  # FP16\n",
    "\n",
    "        # Build transform functions\n",
    "        self.device = device\n",
    "        self.image_size = (256, 128)\n",
    "        self.pixel_mean = [0.485, 0.456, 0.406]\n",
    "        self.pixel_std = [0.229, 0.224, 0.225]\n",
    "        self.transforms = []\n",
    "        self.transforms += [T.Resize(self.image_size)]\n",
    "        self.transforms += [T.ToTensor()]\n",
    "        self.transforms += [T.Normalize(mean=self.pixel_mean, std=self.pixel_std)]\n",
    "        self.preprocess = T.Compose(self.transforms)\n",
    "        self.to_pil = T.ToPILImage()\n",
    "        model_name = get_model_name(w)\n",
    "        print(model_name)\n",
    "\n",
    "        if w.suffix == \".pt\" or w.suffix == '.ckpt':\n",
    "            model_url = get_model_url(w)\n",
    "            if not file_exists(w) and model_url is not None:\n",
    "                gdown.download(model_url, str(w), quiet=False)\n",
    "            elif file_exists(w):\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"No URL associated to the chosen StrongSORT weights ({w}). Choose between:\")\n",
    "                show_downloadeable_models()\n",
    "                exit()\n",
    "\n",
    "        # Build model\n",
    "        self.model = build_model(model_name, num_classes=1, pretrained=not (w and w.is_file()), use_gpu=device)\n",
    "        \n",
    "        if self.ckpt:   # PyTorch\n",
    "            if w and w.is_file() and w.suffix == '.ckpt':\n",
    "                load_pretrained_weights(self.model, w)\n",
    "\n",
    "            self.model.to(device).eval()\n",
    "            self.model.half() if self.fp16 else self.model.float()\n",
    "        else:\n",
    "            print(\"This model framework is not supported yet!\")\n",
    "            exit()\n",
    "    \n",
    "    def _preprocess(self, im_batch):\n",
    "\n",
    "        images = []\n",
    "        for element in im_batch:\n",
    "            image = self.to_pil(element)\n",
    "            image = self.preprocess(image)\n",
    "            images.append(image)\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "        images = images.to(self.device)\n",
    "\n",
    "        return images\n",
    "    \n",
    "    def forward(self, im_batch):\n",
    "\n",
    "        # preprocess batch\n",
    "        im_batch = self._preprocess(im_batch)\n",
    "\n",
    "        # batch to half\n",
    "        if self.fp16 and im_batch.dtype != torch.float16:\n",
    "            im_batch = im_batch.half()\n",
    "\n",
    "        # batch processing\n",
    "        features = []\n",
    "        features = self.model(im_batch)\n",
    "\n",
    "        if isinstance(features, (list, tuple)):\n",
    "            return self.from_numpy(features[0]) if len(features) == 1 else [self.from_numpy(x) for x in features]\n",
    "        else:\n",
    "            return self.from_numpy(features)\n",
    "\n",
    "    def from_numpy(self, x):\n",
    "        return torch.from_numpy(x).to(self.device) if isinstance(x, np.ndarray) else x\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def model_type(p=\"path/to/model.pt\"):\n",
    "        # Return model type from model path, i.e. path='path/to/model.onnx' -> type=onnx\n",
    "        suffixes = list(export_formats().Suffix) + [\".xml\"]  # export suffixes\n",
    "        check_suffix(p, suffixes)  # checks\n",
    "        p = Path(p).name  # eliminate trailing separators\n",
    "        pt, ckpt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, _, xml2 = (s in p for s in suffixes)\n",
    "        xml |= xml2  # *_openvino_model or *.xml\n",
    "        tflite &= not edgetpu  # *.tflite\n",
    "        return pt, ckpt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_base_patch16_224_TransReID\n",
      "using stride: 16, and patch number is num_y16 * num_x8\n",
      "using drop_out rate is : 0.0\n",
      "using attn_drop_out rate is : 0.0\n",
      "using drop_path rate is : 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/son/Desktop/VIN/CV/project/tracker/strongsort/deep/reid_model_factory.py:163: UserWarning: The pretrained weights \"/home/son/Desktop/VIN/CV/project/weights/vit_base_patch16_224_TransReID.ckpt\" cannot be loaded, please check the key names manually (** ignored and continue **)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x  = ReIDDetectMultiBackend(weights=Path('/home/son/Desktop/VIN/CV/project/weights/vit_base_patch16_224_TransReID.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be Tensor or ndarray. Got <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x\u001b[39m.\u001b[39;49mforward(\u001b[39m'\u001b[39;49m\u001b[39m/home/son/Desktop/VIN/CV/project/data/MOT20/train/MOT20-01/img1/000002.jpg\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[30], line 80\u001b[0m, in \u001b[0;36mReIDDetectMultiBackend.forward\u001b[0;34m(self, im_batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, im_batch):\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m     \u001b[39m# preprocess batch\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     im_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess(im_batch)\n\u001b[1;32m     82\u001b[0m     \u001b[39m# batch to half\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mand\u001b[39;00m im_batch\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mfloat16:\n",
      "Cell \u001b[0;32mIn[30], line 68\u001b[0m, in \u001b[0;36mReIDDetectMultiBackend._preprocess\u001b[0;34m(self, im_batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m images \u001b[39m=\u001b[39m []\n\u001b[1;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m im_batch:\n\u001b[0;32m---> 68\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_pil(element)\n\u001b[1;32m     69\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(image)\n\u001b[1;32m     70\u001b[0m     images\u001b[39m.\u001b[39mappend(image)\n",
      "File \u001b[0;32m~/miniconda3/envs/vin/lib/python3.8/site-packages/torchvision/transforms/transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    226\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_pil_image(pic, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n",
      "File \u001b[0;32m~/miniconda3/envs/vin/lib/python3.8/site-packages/torchvision/transforms/functional.py:262\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    259\u001b[0m     _log_api_usage_once(to_pil_image)\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(pic, torch\u001b[39m.\u001b[39mTensor) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(pic, np\u001b[39m.\u001b[39mndarray)):\n\u001b[0;32m--> 262\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be Tensor or ndarray. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pic)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    264\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pic, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mndimension() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m}:\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'str'>."
     ]
    }
   ],
   "source": [
    "x.forward('/home/son/Desktop/VIN/CV/project/data/MOT20/train/MOT20-01/img1/000002.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/son/Desktop/VIN/CV/project/tracker/strongsort/deep/reid_model_factory.py:163: UserWarning: The pretrained weights \"/home/son/Desktop/VIN/CV/project/weights/vit_base_patch16_224_TransReID.ckpt\" cannot be loaded, please check the key names manually (** ignored and continue **)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "load_pretrained_weights(x, '/home/son/Desktop/VIN/CV/project/weights/vit_base_patch16_224_TransReID.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
